{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(filepath):\n",
    "    df = pd.read_csv(filepath, sep='\\t').drop(columns=['name'])\n",
    "    return df\n",
    "\n",
    "def create_edgelist_from_csv(input_csv_filepath,\n",
    "                             lfm1b_user_info_filepath):\n",
    "    lfm1b_user_info_df = load_dataframe(lfm1b_user_info_filepath)\n",
    "    lfm1b_users_df = pd.read_csv(input_csv_filepath, sep='\\t', header=None).drop([2], axis=1).rename(columns={0: 'user1', 1: 'user2'})\n",
    "    lfm1b_users_df = lfm1b_users_df.rename(columns={'user1':'username'})\n",
    "    lfm1b_users_df = lfm1b_users_df.merge(lfm1b_user_info_df[['username', 'user_id']], how='left', on='username').drop(['username'], axis=1)\n",
    "    lfm1b_users_df = lfm1b_users_df.rename(columns={'user2':'username', 'user_id':'user1_id'})\n",
    "    lfm1b_users_df = lfm1b_users_df.merge(lfm1b_user_info_df[['username', 'user_id']], how='left', on='username').drop(['username'], axis=1)\n",
    "    lfm1b_users_df = lfm1b_users_df.rename(columns={'user_id':'user2_id'})\n",
    "    lfm1b_users_df = lfm1b_users_df[~lfm1b_users_df.isin([np.nan, np.inf, -np.inf]).any(1)].reset_index(drop=True).astype('int32')\n",
    "    return lfm1b_users_df\n",
    "\n",
    "def create_graph(edgelist_df):\n",
    "    G = nx.Graph()\n",
    "    for edge in list(zip(edgelist_df.user1_id, edgelist_df.user2_id)):\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "    return G\n",
    "\n",
    "def create_random_edges_dict(G, n):\n",
    "    random_edges_dict = {}\n",
    "    nodes_list = list(G.nodes)\n",
    "    edges_list = list(G.edges)\n",
    "    negative_samples_percentage = 0.001\n",
    "    num_negative_samples = int(negative_samples_percentage*(len(nodes_list)*len(nodes_list)/2 - len(edges_list)))\n",
    "    print(\"Number of negative samples in the random graph:\" + str(num_negative_samples))\n",
    "    for i in range(n):\n",
    "        random_edges = []    \n",
    "        while len(random_edges) < num_negative_samples:\n",
    "            edge = random.sample(nodes_list, 2)\n",
    "            if edge[0]==edge[1] or (edge[0], edge[1]) in random_edges or (edge[1], edge[0]) in random_edges or (edge[0], edge[1]) in edges_list or (edge[1], edge[0]) in edges_list:\n",
    "                continue\n",
    "            random_edges.append((edge[0], edge[1]))\n",
    "        random_edges_dict[i] = random_edges\n",
    "    return random_edges_dict\n",
    "\n",
    "# Create additional features\n",
    "def calculate_relative_change_for_numeric_features(df, columns):\n",
    "    for column in columns:\n",
    "        df['relative_change_'+column] = df.apply(lambda x: relative_diff(x['user1_'+column], x['user2_'+column]), axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_cosine_similarity_for_vector_features(df, columns):\n",
    "    for column in columns:\n",
    "        df['cosine_similarity_'+column] = df.apply(lambda x: cosine_similarity(x['user1_'+column], x['user2_'+column]), axis=1)\n",
    "    return df\n",
    "\n",
    "def create_mask_for_categoric_features(df, columns):\n",
    "    for column in columns:\n",
    "        df['same_'+column] = (df['user1_'+column]==df['user2_'+column]).astype(int)\n",
    "    return df\n",
    "\n",
    "def calculate_jaccard_coefficient(df, G):\n",
    "    df['jaccard_coefficient'] = df.apply(lambda x: get_jaccard_coefficient(G, x['user1_user_id'], x['user2_user_id']), axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_adamic_adar_coefficient(df, G):\n",
    "    df['adamic_adar_coefficient'] = df.apply(lambda x: get_adamic_adar_index(G, x['user1_user_id'], x['user2_user_id']), axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_common_neighbors(df, G):\n",
    "    df['common_neighbors'] = df.apply(lambda x: get_common_neighbors(G, x['user1_user_id'], x['user2_user_id']), axis=1)\n",
    "    return df\n",
    "\n",
    "def relative_diff(x, y):\n",
    "    if x is None or y is None or x==-1 or y==-1:\n",
    "        return None\n",
    "    if max(x,y) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return abs((abs(x)-abs(y))/max(x,y))\n",
    "\n",
    "def cosine_similarity(list1, list2):\n",
    "    return 1 - spatial.distance.cosine(list1, list2)\n",
    "\n",
    "def get_jaccard_coefficient(G, user1, user2):\n",
    "    return list(nx.jaccard_coefficient(G, [(user1, user2)]))[0][2]\n",
    "\n",
    "def get_adamic_adar_index(G, user1, user2):\n",
    "    return list(nx.adamic_adar_index(G, [(user1, user2)]))[0][2]\n",
    "\n",
    "def get_common_neighbors(G, user1, user2):\n",
    "    return len(list(nx.common_neighbors(G, user1, user2)))\n",
    "\n",
    "def extend_users_df_with_relational_features(users_df):\n",
    "    users_df['freebase_nmf_array'] = users_df[['freebase_nmf_0', 'freebase_nmf_1', 'freebase_nmf_2', 'freebase_nmf_3', 'freebase_nmf_4', 'freebase_nmf_5', 'freebase_nmf_6', 'freebase_nmf_7', 'freebase_nmf_8', 'freebase_nmf_9', 'freebase_nmf_10', 'freebase_nmf_11', 'freebase_nmf_12', 'freebase_nmf_13', 'freebase_nmf_14', 'freebase_nmf_15', 'freebase_nmf_16', 'freebase_nmf_17', 'freebase_nmf_18', 'freebase_nmf_19']].values.tolist()\n",
    "    users_df['allmusic_genre_array'] = users_df[['allmusic_rnb', 'allmusic_rap', 'allmusic_electronic', 'allmusic_rock', 'allmusic_new age', 'allmusic_classical', 'allmusic_reggae', 'allmusic_blues', 'allmusic_country', 'allmusic_world', 'allmusic_folk', 'allmusic_easy listening', 'allmusic_jazz', 'allmusic_vocal', \"allmusic_children's\", 'allmusic_punk', 'allmusic_alternative', 'allmusic_spoken word', 'allmusic_pop', 'allmusic_heavy metal']].values.tolist()\n",
    "    users_df['UAM_nmf_array'] = users_df[['UAM_nmf_0', 'UAM_nmf_1', 'UAM_nmf_2', 'UAM_nmf_3', 'UAM_nmf_4', 'UAM_nmf_5', 'UAM_nmf_6', 'UAM_nmf_7', 'UAM_nmf_8', 'UAM_nmf_9', 'UAM_nmf_10', 'UAM_nmf_11', 'UAM_nmf_12', 'UAM_nmf_13', 'UAM_nmf_14', 'UAM_nmf_15', 'UAM_nmf_16', 'UAM_nmf_17', 'UAM_nmf_18', 'UAM_nmf_19']].values.tolist()\n",
    "    users_df['allmusic_genre_array'] = users_df['allmusic_genre_array'].apply(lambda x: np.array(x))\n",
    "    users_df['freebase_nmf_array'] = users_df['freebase_nmf_array'].apply(lambda x: np.array(x))\n",
    "    users_df['UAM_nmf_array'] = users_df['UAM_nmf_array'].apply(lambda x: np.array(x))\n",
    "    return users_df\n",
    "\n",
    "def create_links_df(G, random_edges, users_df):\n",
    "    edges_list = list(G.edges)\n",
    "    users1_df = pd.DataFrame([x[0] for x in edges_list]).rename(columns={0:'user_id'}).merge(users_df, how='left', on='user_id').add_prefix('user1_')\n",
    "    users2_df = pd.DataFrame([x[1] for x in edges_list]).rename(columns={0:'user_id'}).merge(users_df, how='left', on='user_id').add_prefix('user2_')\n",
    "    friendship_df = pd.concat([users1_df, users2_df.set_index(users1_df.index)], axis=1)\n",
    "    friendship_df['label'] = 1\n",
    "    users1_df = pd.DataFrame([x[0] for x in random_edges]).rename(columns={0:'user_id'}).merge(users_df, how='left', on='user_id').add_prefix('user1_')\n",
    "    users2_df = pd.DataFrame([x[1] for x in random_edges]).rename(columns={0:'user_id'}).merge(users_df, how='left', on='user_id').add_prefix('user2_')\n",
    "    no_friendship_df = pd.concat([users1_df, users2_df.set_index(users1_df.index)], axis=1)\n",
    "    no_friendship_df['label'] = 0\n",
    "    links_df = pd.concat([friendship_df, no_friendship_df], axis=0, ignore_index=True)\n",
    "    links_df = calculate_relative_change_for_numeric_features(links_df, ['age','playcount_lognorm','novelty_artist_avg_month','novelty_artist_avg_6months','novelty_artist_avg_year','mainstreaminess_avg_month','mainstreaminess_avg_6months','mainstreaminess_avg_year','mainstreaminess_global','relative_le_per_weekday1','relative_le_per_weekday2','relative_le_per_weekday3','relative_le_per_weekday4','relative_le_per_weekday5','relative_le_per_weekday6','relative_le_per_weekday7','relative_le_per_hour0','relative_le_per_hour1','relative_le_per_hour2','relative_le_per_hour3','relative_le_per_hour4','relative_le_per_hour5','relative_le_per_hour6','relative_le_per_hour7','relative_le_per_hour8','relative_le_per_hour9','relative_le_per_hour10','relative_le_per_hour11','relative_le_per_hour12','relative_le_per_hour13','relative_le_per_hour14','relative_le_per_hour15','relative_le_per_hour16','relative_le_per_hour17','relative_le_per_hour18','relative_le_per_hour19','relative_le_per_hour20','relative_le_per_hour21','relative_le_per_hour22','relative_le_per_hour23','cnt_listeningevents_lognorm','cnt_distinct_tracks_lognorm','cnt_distinct_artists_lognorm','cnt_listeningevents_per_week_lognorm','allmusic_weighted_average_diversity','allmusic_genre_coverage_diversity','allmusic_entropy_diversity','freebase_weighted_average_diversity','freebase_genre_coverage_diversity','freebase_entropy_diversity'])\n",
    "    links_df = create_mask_for_categoric_features(links_df, ['country','gender','age_group','user_groups_freebase_weighted_average_diversity','user_groups_freebase_genre_coverage_diversity','user_groups_freebase_entropy_diversity','user_groups_allmusic_weighted_average_diversity','user_groups_allmusic_genre_coverage_diversity','user_groups_allmusic_entropy_diversity','user_groups_cnt_listeningevents_lognorm','user_groups_cnt_distinct_tracks_lognorm','user_groups_cnt_distinct_artists_lognorm','user_groups_cnt_listeningevents_per_week_lognorm','user_groups_playcount_lognorm','user_groups_novelty_artist_avg_month','user_groups_novelty_artist_avg_6months','user_groups_novelty_artist_avg_year','user_groups_mainstreaminess_avg_month','user_groups_mainstreaminess_avg_6months','user_groups_mainstreaminess_avg_year','user_groups_mainstreaminess_global'])\n",
    "    links_df = calculate_cosine_similarity_for_vector_features(links_df, ['allmusic_genre_array', 'freebase_nmf_array', 'UAM_nmf_array'])\n",
    "    links_df = links_df.select_dtypes(exclude=['object'])\n",
    "    links_df = calculate_jaccard_coefficient(links_df, G)\n",
    "    links_df = calculate_adamic_adar_coefficient(links_df, G)\n",
    "    links_df = calculate_common_neighbors(links_df, G)\n",
    "    return links_df\n",
    "\n",
    "def create_and_store_dfs(G, random_edges_dict, users_df):\n",
    "    for i in range(len(random_edges_dict)):\n",
    "        links_df = create_links_df(G, random_edges_dict[i], users_df)\n",
    "        outdir = '../data/dataframes/links_dfs/'+ str(i)\n",
    "        if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "        links_df.to_csv(outdir + '/links_df_full.csv')\n",
    "        network_features_df = links_df[['user1_user_id', 'user2_user_id', 'jaccard_coefficient', 'adamic_adar_coefficient', 'common_neighbors', 'label']]\n",
    "        demographic_features_df = pd.concat([links_df.filter(regex=(\"user1_user_id\")), links_df.filter(regex=(\"user2_user_id\")), links_df.filter(regex=(\".*gender.*\")), links_df.filter(regex=(\".*age.*\")), links_df.filter(regex=(\".*country.*\")), links_df.label], axis=1).drop(columns=['user1_allmusic_country', 'user2_allmusic_country'])\n",
    "        demographic_features_df = demographic_features_df[demographic_features_df.columns.drop(list(demographic_features_df.filter(regex='allmusic')))]\n",
    "        demographic_features_df = demographic_features_df[demographic_features_df.columns.drop(list(demographic_features_df.filter(regex='freebase')))]\n",
    "        genre_features_df = pd.concat([links_df.filter(regex=(\"user1_user_id\")), links_df.filter(regex=(\"user2_user_id\")), links_df.filter(regex=(\".*allmusic.*\")), links_df.filter(regex=(\".*freebase.*\")), links_df.label], axis=1)\n",
    "        genre_features_df = genre_features_df[genre_features_df.columns.drop(list(genre_features_df.filter(regex='diversity')))]\n",
    "        listening_profile_features_df = pd.concat([links_df.filter(regex=(\"user1_user_id\")), links_df.filter(regex=(\"user2_user_id\")), links_df.filter(regex=(\".*UAM.*\")), links_df.label], axis=1)\n",
    "        listening_characteristics_features_df = pd.concat([links_df.filter(regex=(\"user1_user_id\")), links_df.filter(regex=(\"user2_user_id\")), links_df.filter(regex=(\".*mainstreaminess.*\")), links_df.filter(regex=(\".*novelty.*\")), links_df.filter(regex=(\".*diversity.*\")), links_df.filter(regex=(\".*cnt.*\")), links_df.filter(regex=(\".*playcount.*\")), links_df.label], axis=1)\n",
    "        network_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_network_features.csv')\n",
    "        demographic_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_demographic_features.csv')\n",
    "        genre_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_genre_features.csv')\n",
    "        listening_profile_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_listening_profile_features.csv')\n",
    "        listening_characteristics_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_listening_characteristics_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "lfm1b_user_info_filepath = '../data/raw/LFM-1b_social/LFM-1b_users_noname.txt'\n",
    "lfm1b_user_additional_info_filepath = '../data/raw/LFM-1b/LFM-1b_users_additional.txt'\n",
    "lfm1b_user_genres_allmusic_filepath = '../data/raw/LFM-1b_UGP/LFM-1b_UGP_weightedPC_allmusic.txt'\n",
    "lfm1b_user_genres_freebase_filepath = '../data/raw/LFM-1b_UGP/LFM-1b_UGP_weightedPC_freebase.txt'\n",
    "input_edgelist_csv_filepath = '../data/raw/LFM-1b_social/LFM-1b_social_ties.txt'\n",
    "lfm1b_user_artists_LEs_filepath = '../data/raw/LFM-1b/LFM-1b_LEs.mat'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples in the random graph:69446\n"
     ]
    }
   ],
   "source": [
    "lfm1b_users_df = load_dataframe(lfm1b_user_info_filepath)\n",
    "edgelist_df = create_edgelist_from_csv(input_edgelist_csv_filepath, lfm1b_user_info_filepath)\n",
    "G = create_graph(edgelist_df)\n",
    "\n",
    "# Nunber of datasets with random negative samples to create, 1 when experimenting, 10 for robust evaluation\n",
    "num_datasets = 1\n",
    "random_edges_dict = create_random_edges_dict(G, num_datasets)\n",
    "\n",
    "users_df = pd.read_csv('../data/dataframes/users_df.csv', index_col=0)\n",
    "users_df = extend_users_df_with_relational_features(users_df)\n",
    "# Not using this in later study since xgboost can work with missing values\n",
    "# users_df_without_missing_values = pd.read_csv('../data/dataframes/users_df_no_missing_values.csv', index_col=0)\n",
    "# users_df_without_missing_values = extend_users_df_with_relational_features(users_df_without_missing_values)\n",
    "\n",
    "create_and_store_dfs(G, random_edges_dict, users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lala\n"
     ]
    }
   ],
   "source": [
    "print('lala')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [d for v, d in G.degree()]\n",
    "G_conf=nx.configuration_model(degrees)\n",
    "G_conf=nx.Graph(G_conf)\n",
    "G_conf.remove_edges_from(nx.selfloop_edges(G_conf))\n",
    "\n",
    "node_ids = list(G.nodes)\n",
    "source_nodes = []\n",
    "target_nodes = []\n",
    "for edge in G_conf.edges:\n",
    "    source_nodes.append(node_ids[edge[0]])\n",
    "    target_nodes.append(node_ids[edge[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1_df = pd.DataFrame(source_nodes).rename(columns={0:'user_id'}).merge(users_df[['user_id', 'UAM_nmf_array', 'freebase_nmf_array']], on='user_id', how='left').rename(columns={'user_id':'user1_user_id', 'UAM_nmf_array':'user1_UAM_nmf_array', 'freebase_nmf_array':'user1_freebase_array'})\n",
    "user2_df = pd.DataFrame(target_nodes).rename(columns={0:'user_id'}).merge(users_df[['user_id', 'UAM_nmf_array', 'freebase_nmf_array']], on='user_id', how='left').rename(columns={'user_id':'user2_user_id', 'UAM_nmf_array':'user2_UAM_nmf_array', 'freebase_nmf_array':'user2_freebase_array'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewired_graph_df = pd.concat([user1_df, user2_df], axis=1)\n",
    "rewired_graph_df['cosine_similarity_'+'UAM_nmf_array'] = rewired_graph_df.apply(lambda x: cosine_similarity(x['user1_'+'UAM_nmf_array'], x['user2_'+'UAM_nmf_array']), axis=1)\n",
    "rewired_graph_df['cosine_similarity_'+'freebase_array'] = rewired_graph_df.apply(lambda x: cosine_similarity(x['user1_'+'freebase_array'], x['user2_'+'freebase_array']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataframes/users_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# keep only positive samples\u001B[39;00m\n\u001B[1;32m      3\u001B[0m links_df \u001B[38;5;241m=\u001B[39m links_df[links_df\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m----> 5\u001B[0m users_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdataframes/users_df.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnnamed: 0\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      6\u001B[0m edges \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(links_df\u001B[38;5;241m.\u001B[39muser1_user_id, links_df\u001B[38;5;241m.\u001B[39muser2_user_id))\n\u001B[1;32m      7\u001B[0m G \u001B[38;5;241m=\u001B[39m nx\u001B[38;5;241m.\u001B[39mGraph()\n",
      "File \u001B[0;32m~/01_Development/tools/anaconda3/envs/lastfm/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[1;32m    310\u001B[0m     )\n\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/01_Development/tools/anaconda3/envs/lastfm/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    665\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    666\u001B[0m     dialect,\n\u001B[1;32m    667\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    676\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m    677\u001B[0m )\n\u001B[1;32m    678\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/01_Development/tools/anaconda3/envs/lastfm/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    572\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    574\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 575\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/01_Development/tools/anaconda3/envs/lastfm/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    930\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    932\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 933\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/01_Development/tools/anaconda3/envs/lastfm/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1213\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1214\u001B[0m \u001B[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001B[39;00m\n\u001B[1;32m   1215\u001B[0m \u001B[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[39;00m\n\u001B[1;32m   1216\u001B[0m \u001B[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[39;00m\n\u001B[0;32m-> 1217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[call-overload]\u001B[39;49;00m\n\u001B[1;32m   1218\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1222\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1224\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1225\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1226\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1227\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1228\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/01_Development/tools/anaconda3/envs/lastfm/lib/python3.9/site-packages/pandas/io/common.py:789\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    784\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    785\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    786\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    787\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    788\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    797\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    798\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'dataframes/users_df.csv'"
     ]
    }
   ],
   "source": [
    "links_df = pd.read_csv('../data/dataframes/links_dfs/0/links_df_full.csv', index_col=False).drop(columns=['Unnamed: 0'])\n",
    "# keep only positive samples\n",
    "links_df = links_df[links_df.label==1]\n",
    "\n",
    "users_df = pd.read_csv('dataframes/users_df.csv', index_col=False).drop(columns=['Unnamed: 0'])\n",
    "edges = list(zip(links_df.user1_user_id, links_df.user2_user_id))\n",
    "G = nx.Graph()\n",
    "for edge in edges:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "\n",
    "# if you want to look only into users in the LCC\n",
    "#G_lcc = [G.subgraph(c).copy() for c in nx.connected_components(G)][1]\n",
    "#users_df = users_df[users_df['user_id'].isin(G_lcc.nodes)]\n",
    "#links_df = links_df[(links_df['user1_user_id'].isin(G_lcc.nodes))&(links_df['user2_user_id'].isin(G_lcc.nodes))]\n",
    "#df_subset = links_df[['cosine_similarity_freebase_nmf_array', 'cosine_similarity_UAM_nmf_array']]\n",
    "\n",
    "users_df = users_df[users_df['user_id'].isin(G.nodes)]\n",
    "links_df = links_df[(links_df['user1_user_id'].isin(G.nodes))&(links_df['user2_user_id'].isin(G.nodes))]\n",
    "df_subset = links_df[['cosine_similarity_freebase_nmf_array', 'cosine_similarity_UAM_nmf_array']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pd.Series(df_subset['cosine_similarity_UAM_nmf_array'].values, name='Listening profile similarity'), color='blue', label='Existing edges', kde=True, norm_hist=True)\n",
    "sns.distplot(rewired_graph_df['cosine_similarity_UAM_nmf_array'].values, color='black', label='Randomly rewired edges', kde=True, norm_hist=True)\n",
    "plt.xlim(0, 1.0)\n",
    "plt.legend();\n",
    "plt.savefig('../figures/existing_vs_random_listening_profile_similarity.pdf',dpi=300, bbox_inches = \"tight\")\n",
    "print(df_subset['cosine_similarity_UAM_nmf_array'].mean())\n",
    "print(df_subset['cosine_similarity_UAM_nmf_array'].std())\n",
    "print(rewired_graph_df['cosine_similarity_UAM_nmf_array'].mean())\n",
    "print(rewired_graph_df['cosine_similarity_UAM_nmf_array'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pd.Series(df_subset['cosine_similarity_freebase_nmf_array'].values, name='Genre profile similarity'), color='blue', label='Existing edges', kde=True, norm_hist=True)\n",
    "sns.distplot(rewired_graph_df['cosine_similarity_freebase_array'].values, color='black', label='Randomly rewired edges', kde=True, norm_hist=True)\n",
    "plt.xlim(0, 1.0)\n",
    "plt.legend();\n",
    "plt.savefig('figures/existing_vs_random_genre_profile_similarity.pdf',dpi=300, bbox_inches = \"tight\")\n",
    "print(df_subset['cosine_similarity_freebase_nmf_array'].mean())\n",
    "print(df_subset['cosine_similarity_freebase_nmf_array'].std())\n",
    "print(rewired_graph_df['cosine_similarity_freebase_array'].mean())\n",
    "print(rewired_graph_df['cosine_similarity_freebase_array'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}